{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Exploration - Reed API #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will connect to the Reed API and request some data this will allow me to view the format of the data reuturnded from the API, from here I can decide what data is relevant for the **extract** stage of the **ETL** process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the API ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os.path\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a function that makes a call to the Reed API and returns the results as a list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_jobs(keywords, locationName, distance_in_miles):\n",
    "\n",
    "\n",
    "    api_key = \"440e3704-d856-440c-b07f-00ec507efd09\"\n",
    "    username = api_key\n",
    "    password = \"\"\n",
    "\n",
    "    url = f\"https://www.reed.co.uk/api/1.0/search?keywords={keywords}&locationName={locationName}&distanceFromLocation={distance_in_miles}\"\n",
    "\n",
    "    response = requests.get(url,\n",
    "                        auth=requests.auth.HTTPBasicAuth(\n",
    "                            username, password))\n",
    "\n",
    "    json_data = json.loads(response.text)\n",
    "    return json_data['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request Data ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for all job listings that contain the keyword 'data' and save the result to the data variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = search_jobs(\"data\",\"\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Data ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to the number of jobs returned from the response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears the API response is limited to 100 results and there is no pagination within the response, as I will need more than 100 results for this project I will instead be more specific with my API request.  \n",
    "My idea is to create a list of UK cities and run a seperate API requests for each city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "uk_cities = ['London',\n",
    " 'Birmingham',\n",
    " 'Manchester',\n",
    " 'Glasgow',\n",
    " 'Leeds',\n",
    " 'Liverpool',\n",
    " 'Bristol',\n",
    " 'Sheffield',\n",
    " 'Edinburgh',\n",
    " 'Leicester',\n",
    " 'Coventry',\n",
    " 'Nottingham',\n",
    " 'Hull',\n",
    " 'Stoke-on-Trent',\n",
    " 'Derby',\n",
    " 'Southampton',\n",
    " 'Reading',\n",
    " 'Luton',\n",
    " 'Wolverhampton',\n",
    " 'Belfast',\n",
    " 'Cambridge',\n",
    " 'Brighton',\n",
    " 'Oxford',\n",
    " 'Newcastle',\n",
    " 'Cardiff',\n",
    " 'York']\n",
    "\n",
    "len(uk_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London 100\n",
      "Birmingham 100\n",
      "Manchester 100\n",
      "Glasgow 37\n",
      "Leeds 92\n",
      "Liverpool 74\n",
      "Bristol 100\n",
      "Sheffield 59\n",
      "Edinburgh 32\n",
      "Leicester 51\n",
      "Coventry 100\n",
      "Nottingham 84\n",
      "Hull 18\n",
      "Stoke-on-Trent 55\n",
      "Derby 52\n",
      "Southampton 75\n",
      "Reading 100\n",
      "Luton 76\n",
      "Wolverhampton 52\n",
      "Belfast 8\n",
      "Cambridge 42\n",
      "Brighton 26\n",
      "Oxford 52\n",
      "Newcastle 62\n",
      "Cardiff 31\n",
      "York 23\n"
     ]
    }
   ],
   "source": [
    "for city in uk_cities:\n",
    "    city_results = search_jobs(\"data engineer\", city, 10)\n",
    "    print(city, len(city_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Brighton = search_jobs(\"data engineer\", \"Brighton\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jobId': 53990627,\n",
       " 'employerId': 617997,\n",
       " 'employerName': 'In Technology Group Limited',\n",
       " 'employerProfileId': None,\n",
       " 'employerProfileName': None,\n",
       " 'jobTitle': 'Analytics Consultant',\n",
       " 'locationName': 'Brighton',\n",
       " 'minimumSalary': 55000.0,\n",
       " 'maximumSalary': 75000.0,\n",
       " 'currency': 'GBP',\n",
       " 'expirationDate': '29/11/2024',\n",
       " 'date': '08/11/2024',\n",
       " 'jobDescription': 'Title: Analytics Consultant (GCP) Salary: 55,000 - 75,000 Bonus Location: Brighton / Fully Remote About Us The client is a consultancy based in Brighton. They work with organisations to modernise and scale their data analytics capabilities based on a modern data stack based on Google Cloud, DBT Labs, Looker, Cube, Dagster, and other partner technology. We work with our clients to design, build and support innovative analytics solutions that empo... ',\n",
       " 'applications': 27,\n",
       " 'jobUrl': 'https://www.reed.co.uk/jobs/analytics-consultant/53990627'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Brighton[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further increase the number of data jobs listed I will search for different tech job titles in each city. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tech_job_titles = [\n",
    "    \"Data Scientist\",\n",
    "    \"Data Analyst\",\n",
    "    \"Data Engineer\",\n",
    "    \"Cloud Engineer\",\n",
    "    \"Software Engineer\",\n",
    "    \"Software Developer\",\n",
    "    \"Frontend Developer\",\n",
    "    \"Backend Developer\",\n",
    "    \"Full Stack Developer\",\n",
    "    \"DevOps Engineer\",\n",
    "    \"Machine Learning Engineer\",\n",
    "    \"AI Engineer\",\n",
    "    \"Mobile App Developer\",\n",
    "    \"Game Developer\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_tech_job_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have created a list of 14 different job titles, I will use the API to search each job title in each city in my uk_cities list. This should extract enough data to perform a good analysis. \n",
    "\n",
    "The next stage is the **extract** phase of the **ETL** pipeline in which we will extract the data from the API and store it in a pandas dataframe ready for **transformation**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
